{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Set paths\n",
    "datasets_dir = '../../../datasets'\n",
    "vqax_dir = os.path.join(datasets_dir, 'VQA-X')\n",
    "train_dir = os.path.join(vqax_dir, 'vqaX_train.json')\n",
    "test_dir = os.path.join(vqax_dir, 'vqaX_test.json')\n",
    "val_dir = os.path.join(vqax_dir, 'vqaX_val.json')\n",
    "with open(train_dir) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(test_dir) as f:\n",
    "    test_data = json.load(f)\n",
    "with open(val_dir) as f:\n",
    "    val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"What is this?\",\n",
      "  \"answers\": [\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 1\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 2\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 3\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 4\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"maybe\",\n",
      "      \"answer_id\": 5\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 6\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 7\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 8\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 9\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 10\n",
      "    }\n",
      "  ],\n",
      "  \"image_id\": \"262284\",\n",
      "  \"image_name\": \"COCO_val2014_000000262284.jpg\",\n",
      "  \"explanation\": [\n",
      "    \"it has a shower head hanging inside of it\",\n",
      "    \"there is a shower head\",\n",
      "    \"there is a faucet and a bathtub\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(test_data['262284001'],indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing vqaX_train_ggtrans.json...\n",
      "Total items: 29459\n",
      "Empty translations:\n",
      "Total items with at least one empty translation: 0\n",
      "\n",
      "\n",
      "Analyzing vqaX_test_ggtrans.json...\n",
      "Total items: 1968\n",
      "Empty translations:\n",
      "Total items with at least one empty translation: 0\n",
      "\n",
      "\n",
      "Analyzing vqaX_val_ggtrans.json...\n",
      "Total items: 1459\n",
      "Empty translations:\n",
      "Total items with at least one empty translation: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "data_dir = '../../../datasets/VQA-X'\n",
    "\n",
    "# Danh sách các file cần kiểm tra\n",
    "files_to_check = ['vqaX_train_ggtrans.json', 'vqaX_test_ggtrans.json', 'vqaX_val_ggtrans.json']\n",
    "def analyze_translations(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_items = len(data)\n",
    "    empty_translations = Counter()\n",
    "    error_count = 0\n",
    "    \n",
    "    for key, item in data.items():\n",
    "        if item.get('question_vi_ggtrans', '') == '':\n",
    "            empty_translations['question'] += 1\n",
    "        if item.get('answer_vi_ggtrans', '') == '':\n",
    "            empty_translations['answer'] += 1\n",
    "        if not item.get('explanation_vi_ggtrans', []):\n",
    "            empty_translations['explanation'] += 1\n",
    "        elif '' in item['explanation_vi_ggtrans']:\n",
    "            empty_translations['explanation'] += 1\n",
    "        \n",
    "        # Kiểm tra nếu có bất kỳ trường dịch nào trống\n",
    "        if (item.get('question_vi_ggtrans', '') == '' or\n",
    "            item.get('answer_vi_ggtrans', '') == '' or\n",
    "            not item.get('explanation_vi_ggtrans', []) or \n",
    "            '' in item.get('explanation_vi_ggtrans', [])):\n",
    "            error_count += 1\n",
    "    \n",
    "    return {\n",
    "        'total_items': total_items,\n",
    "        'empty_translations': dict(empty_translations),\n",
    "        'error_count': error_count\n",
    "    }\n",
    "\n",
    "# Phân tích từng file\n",
    "for file_name in files_to_check:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Analyzing {file_name}...\")\n",
    "        results = analyze_translations(file_path)\n",
    "        \n",
    "        print(f\"Total items: {results['total_items']}\")\n",
    "        print(\"Empty translations:\")\n",
    "        for field, count in results['empty_translations'].items():\n",
    "            print(f\"  {field}: {count}\")\n",
    "        print(f\"Total items with at least one empty translation: {results['error_count']}\")\n",
    "        # print(f\"Percentage of items with errors: {results['error_count']/results['total_items']*100:.2f}%\")\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating train dataset...\n",
      "Total items: 29459\n",
      "Valid items: 29413 (99.84%)\n",
      "\n",
      "Error counts:\n",
      "  wrong_explanation_count_vinai: 46 (0.16%)\n",
      "\n",
      "Validating val dataset...\n",
      "Total items: 1459\n",
      "Valid items: 1448 (99.25%)\n",
      "\n",
      "Error counts:\n",
      "  wrong_explanation_count_vinai: 11 (0.75%)\n",
      "\n",
      "Validating test dataset...\n",
      "Total items: 1968\n",
      "Valid items: 1957 (99.44%)\n",
      "\n",
      "Error counts:\n",
      "  wrong_explanation_count_vinai: 11 (0.56%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Đường dẫn đến thư mục chứa các file dữ liệu\n",
    "datasets_dir = '../../../datasets/VQA-X'\n",
    "\n",
    "# Danh sách các tập dữ liệu\n",
    "datasets = ['train', 'val', 'test']\n",
    "\n",
    "# Danh sách các nguồn dịch\n",
    "translation_sources = ['vinai', 'gemini', 'ggtrans']\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def validate_translations(datasets_dir, datasets, translation_sources):\n",
    "    for dataset in datasets:\n",
    "        print(f\"\\nValidating {dataset} dataset...\")\n",
    "        \n",
    "        # Load merged data\n",
    "        merged_file = f'{datasets_dir}/vqaX_{dataset}_translated.json'\n",
    "        merged_data = load_json(merged_file)\n",
    "        \n",
    "        total_items = len(merged_data)\n",
    "        valid_items = 0\n",
    "        error_counts = defaultdict(int)\n",
    "        \n",
    "        for key, item in merged_data.items():\n",
    "            is_valid = True\n",
    "            original_explanation_count = len(item['explanation'])\n",
    "            \n",
    "            for source in translation_sources:\n",
    "                # Check question translation\n",
    "                if f'question_vi_{source}' not in item or not item[f'question_vi_{source}']:\n",
    "                    error_counts[f'missing_question_{source}'] += 1\n",
    "                    is_valid = False\n",
    "                \n",
    "                # Check answer translation\n",
    "                if f'answer_vi_{source}' not in item or not item[f'answer_vi_{source}']:\n",
    "                    error_counts[f'missing_answer_{source}'] += 1\n",
    "                    is_valid = False\n",
    "                \n",
    "                # Check explanation translation\n",
    "                if f'explanation_vi_{source}' not in item:\n",
    "                    error_counts[f'missing_explanation_{source}'] += 1\n",
    "                    is_valid = False\n",
    "                else:\n",
    "                    explanations = item[f'explanation_vi_{source}']\n",
    "                    if len(explanations) != original_explanation_count:\n",
    "                        error_counts[f'wrong_explanation_count_{source}'] += 1\n",
    "                        is_valid = False\n",
    "                    elif any(not exp for exp in explanations):\n",
    "                        error_counts[f'empty_explanation_{source}'] += 1\n",
    "                        is_valid = False\n",
    "            \n",
    "            if is_valid:\n",
    "                valid_items += 1\n",
    "        \n",
    "        print(f\"Total items: {total_items}\")\n",
    "        print(f\"Valid items: {valid_items} ({valid_items/total_items*100:.2f}%)\")\n",
    "        print(\"\\nError counts:\")\n",
    "        for error, count in error_counts.items():\n",
    "            print(f\"  {error}: {count} ({count/total_items*100:.2f}%)\")\n",
    "\n",
    "# Run the validation process\n",
    "validate_translations(datasets_dir, datasets, translation_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n",
      "Merged data saved to ../../../datasets/VQA-X/vqaX_train_translated.json\n",
      "Total items in original data: 29459\n",
      "Total items in merged data: 29459\n",
      "Items with vinai translation: 29459\n",
      "Items with gemini translation: 29459\n",
      "Items with ggtrans translation: 29459\n",
      "\n",
      "\n",
      "Processing val dataset...\n",
      "Merged data saved to ../../../datasets/VQA-X/vqaX_val_translated.json\n",
      "Total items in original data: 1459\n",
      "Total items in merged data: 1459\n",
      "Items with vinai translation: 1459\n",
      "Items with gemini translation: 1459\n",
      "Items with ggtrans translation: 1459\n",
      "\n",
      "\n",
      "Processing test dataset...\n",
      "Merged data saved to ../../../datasets/VQA-X/vqaX_test_translated.json\n",
      "Total items in original data: 1968\n",
      "Total items in merged data: 1968\n",
      "Items with vinai translation: 1968\n",
      "Items with gemini translation: 1968\n",
      "Items with ggtrans translation: 1968\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Đường dẫn đến thư mục chứa các file dữ liệu\n",
    "datasets_dir = '../../../datasets/VQA-X'\n",
    "\n",
    "# Danh sách các tập dữ liệu\n",
    "datasets = ['train', 'val', 'test']\n",
    "\n",
    "# Danh sách các nguồn dịch\n",
    "translation_sources = ['vinai', 'gemini', 'ggtrans']\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def merge_translations(datasets_dir, datasets, translation_sources):\n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing {dataset} dataset...\")\n",
    "        \n",
    "        # Load original data\n",
    "        original_file = f'{datasets_dir}/vqaX_{dataset}.json'\n",
    "        original_data = load_json(original_file)\n",
    "        \n",
    "        # Load translations\n",
    "        translations = {}\n",
    "        for source in translation_sources:\n",
    "            translation_file = f'{datasets_dir}/vqaX_{dataset}_{source}.json'\n",
    "            if os.path.exists(translation_file):\n",
    "                translations[source] = load_json(translation_file)\n",
    "            else:\n",
    "                print(f\"Warning: {translation_file} not found. Skipping this translation source.\")\n",
    "        \n",
    "        # Merge translations\n",
    "        merged_data = {}\n",
    "        for key, item in original_data.items():\n",
    "            merged_item = item.copy()\n",
    "            for source in translation_sources:\n",
    "                if source in translations and key in translations[source]:\n",
    "                    translated_item = translations[source][key]\n",
    "                    merged_item[f'question_vi_{source}'] = translated_item.get(f'question_vi_{source}')\n",
    "                    merged_item[f'answer_vi_{source}'] = translated_item.get(f'answer_vi_{source}')\n",
    "                    merged_item[f'explanation_vi_{source}'] = translated_item.get(f'explanation_vi_{source}')\n",
    "            merged_data[key] = merged_item\n",
    "        \n",
    "        # Save merged data\n",
    "        output_file = f'{datasets_dir}/vqaX_{dataset}_translated.json'\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Merged data saved to {output_file}\")\n",
    "        \n",
    "        # Print some statistics\n",
    "        print(f\"Total items in original data: {len(original_data)}\")\n",
    "        print(f\"Total items in merged data: {len(merged_data)}\")\n",
    "        for source in translation_sources:\n",
    "            if source in translations:\n",
    "                print(f\"Items with {source} translation: {sum(1 for item in merged_data.values() if f'question_vi_{source}' in item)}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Run the merging process\n",
    "merge_translations(datasets_dir, datasets, translation_sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
