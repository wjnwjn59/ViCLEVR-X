{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Set paths\n",
    "datasets_dir = '../../../datasets'\n",
    "vqax_dir = os.path.join(datasets_dir, 'VQA-X')\n",
    "train_dir = os.path.join(vqax_dir, 'vqaX_train.json')\n",
    "test_dir = os.path.join(vqax_dir, 'vqaX_test.json')\n",
    "val_dir = os.path.join(vqax_dir, 'vqaX_val.json')\n",
    "with open(train_dir) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(test_dir) as f:\n",
    "    test_data = json.load(f)\n",
    "with open(val_dir) as f:\n",
    "    val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"What is this?\",\n",
      "  \"answers\": [\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 1\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 2\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 3\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 4\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"maybe\",\n",
      "      \"answer_id\": 5\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 6\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 7\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 8\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 9\n",
      "    },\n",
      "    {\n",
      "      \"answer\": \"shower\",\n",
      "      \"answer_confidence\": \"yes\",\n",
      "      \"answer_id\": 10\n",
      "    }\n",
      "  ],\n",
      "  \"image_id\": \"262284\",\n",
      "  \"image_name\": \"COCO_val2014_000000262284.jpg\",\n",
      "  \"explanation\": [\n",
      "    \"it has a shower head hanging inside of it\",\n",
      "    \"there is a shower head\",\n",
      "    \"there is a faucet and a bathtub\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(test_data['262284001'],indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying the translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking train dataset...\n",
      "Total items: 29459\n",
      "Items with errors: 0\n",
      "\n",
      "\n",
      "Checking val dataset...\n",
      "Total items: 1459\n",
      "Items with errors: 0\n",
      "\n",
      "\n",
      "Checking test dataset...\n",
      "Total items: 1968\n",
      "Items with errors: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Đường dẫn đến thư mục chứa các file dữ liệu\n",
    "datasets_dir = '../../../datasets/VQA-X'\n",
    "\n",
    "# Danh sách các tập dữ liệu\n",
    "datasets = ['train', 'val', 'test']\n",
    "\n",
    "# Danh sách các nguồn dịch\n",
    "translation_sources = ['vinai', 'gemini', 'ggtrans']\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def check_data_integrity(datasets_dir, datasets, translation_sources):\n",
    "    for dataset in datasets:\n",
    "        print(f\"Checking {dataset} dataset...\")\n",
    "        \n",
    "        # Load merged data\n",
    "        merged_file = f'{datasets_dir}/vqaX_{dataset}_translated.json'\n",
    "        merged_data = load_json(merged_file)\n",
    "        \n",
    "        error_items = {}\n",
    "        \n",
    "        for key, item in merged_data.items():\n",
    "            item_errors = []\n",
    "            \n",
    "            # Check original fields\n",
    "            if not item['question'] or not item['answers'] or not item['explanation']:\n",
    "                item_errors.append(\"Missing original fields\")\n",
    "            \n",
    "            # Check translated fields\n",
    "            for source in translation_sources:\n",
    "                question_key = f'question_vi_{source}'\n",
    "                answer_key = f'answer_vi_{source}'\n",
    "                explanation_key = f'explanation_vi_{source}'\n",
    "                \n",
    "                if question_key not in item or not item[question_key]:\n",
    "                    item_errors.append(f\"Missing or empty {question_key}\")\n",
    "                \n",
    "                if answer_key not in item or not item[answer_key]:\n",
    "                    item_errors.append(f\"Missing or empty {answer_key}\")\n",
    "                \n",
    "                if explanation_key not in item:\n",
    "                    item_errors.append(f\"Missing {explanation_key}\")\n",
    "                elif not item[explanation_key]:\n",
    "                    item_errors.append(f\"Empty {explanation_key}\")\n",
    "                elif len(item[explanation_key]) != len(item['explanation']):\n",
    "                    item_errors.append(f\"Mismatch in number of explanations for {explanation_key}\")\n",
    "                elif any(not exp for exp in item[explanation_key]):\n",
    "                    item_errors.append(f\"Empty explanation in {explanation_key}\")\n",
    "            \n",
    "            if item_errors:\n",
    "                error_items[key] = item_errors\n",
    "        \n",
    "        # Save error items\n",
    "        error_file = f'{dataset}_errors.json'\n",
    "        # with open(error_file, 'w', encoding='utf-8') as f:\n",
    "        #     json.dump(error_items, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Total items: {len(merged_data)}\")\n",
    "        print(f\"Items with errors: {len(error_items)}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Run the integrity check\n",
    "check_data_integrity(datasets_dir, datasets, translation_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine translations into one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n",
      "Merged data saved to ../../../datasets/VQA-X/vqaX_train_translated.json\n",
      "Total items in original data: 29459\n",
      "Total items in merged data: 29459\n",
      "Items with vinai translation: 29459\n",
      "Items with gemini translation: 29459\n",
      "Items with ggtrans translation: 29459\n",
      "\n",
      "\n",
      "Processing val dataset...\n",
      "Merged data saved to ../../../datasets/VQA-X/vqaX_val_translated.json\n",
      "Total items in original data: 1459\n",
      "Total items in merged data: 1459\n",
      "Items with vinai translation: 1459\n",
      "Items with gemini translation: 1459\n",
      "Items with ggtrans translation: 1459\n",
      "\n",
      "\n",
      "Processing test dataset...\n",
      "Merged data saved to ../../../datasets/VQA-X/vqaX_test_translated.json\n",
      "Total items in original data: 1968\n",
      "Total items in merged data: 1968\n",
      "Items with vinai translation: 1968\n",
      "Items with gemini translation: 1968\n",
      "Items with ggtrans translation: 1968\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Đường dẫn đến thư mục chứa các file dữ liệu\n",
    "datasets_dir = '../../../datasets/VQA-X'\n",
    "\n",
    "# Danh sách các tập dữ liệu\n",
    "datasets = ['train', 'val', 'test']\n",
    "\n",
    "# Danh sách các nguồn dịch\n",
    "translation_sources = ['vinai', 'gemini', 'ggtrans']\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def merge_translations(datasets_dir, datasets, translation_sources):\n",
    "    for dataset in datasets:\n",
    "        print(f\"Processing {dataset} dataset...\")\n",
    "        \n",
    "        # Load original data\n",
    "        original_file = f'{datasets_dir}/vqaX_{dataset}.json'\n",
    "        original_data = load_json(original_file)\n",
    "        \n",
    "        # Load translations\n",
    "        translations = {}\n",
    "        for source in translation_sources:\n",
    "            translation_file = f'{datasets_dir}/vqaX_{dataset}_{source}.json'\n",
    "            if os.path.exists(translation_file):\n",
    "                translations[source] = load_json(translation_file)\n",
    "            else:\n",
    "                print(f\"Warning: {translation_file} not found. Skipping this translation source.\")\n",
    "        \n",
    "        # Merge translations\n",
    "        merged_data = {}\n",
    "        for key, item in original_data.items():\n",
    "            merged_item = item.copy()\n",
    "            for source in translation_sources:\n",
    "                if source in translations and key in translations[source]:\n",
    "                    translated_item = translations[source][key]\n",
    "                    merged_item[f'question_vi_{source}'] = translated_item.get(f'question_vi_{source}')\n",
    "                    merged_item[f'answer_vi_{source}'] = translated_item.get(f'answer_vi_{source}')\n",
    "                    merged_item[f'explanation_vi_{source}'] = translated_item.get(f'explanation_vi_{source}')\n",
    "            merged_data[key] = merged_item\n",
    "        \n",
    "        # Save merged data\n",
    "        output_file = f'{datasets_dir}/vqaX_{dataset}_translated.json'\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Merged data saved to {output_file}\")\n",
    "        \n",
    "        # Print some statistics\n",
    "        print(f\"Total items in original data: {len(original_data)}\")\n",
    "        print(f\"Total items in merged data: {len(merged_data)}\")\n",
    "        for source in translation_sources:\n",
    "            if source in translations:\n",
    "                print(f\"Items with {source} translation: {sum(1 for item in merged_data.values() if f'question_vi_{source}' in item)}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Run the merging process\n",
    "merge_translations(datasets_dir, datasets, translation_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/binhdt_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "tokenizer_en2vi = AutoTokenizer.from_pretrained(\"vinai/vinai-translate-en2vi-v2\", src_lang=\"en_XX\")\n",
    "model_en2vi = AutoModelForSeq2SeqLM.from_pretrained(\"vinai/vinai-translate-en2vi-v2\")\n",
    "device_en2vi = torch.device(\"cuda\")\n",
    "model_en2vi.to(device_en2vi)\n",
    "\n",
    "def translate_en2vi(en_texts):\n",
    "    input_ids = tokenizer_en2vi(en_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device_en2vi)\n",
    "    output_ids = model_en2vi.generate(\n",
    "        **input_ids,\n",
    "        decoder_start_token_id=tokenizer_en2vi.lang_code_to_id[\"vi_VN\"],\n",
    "        num_return_sequences=1,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    vi_texts = tokenizer_en2vi.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return vi_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dơi']\n",
      "['Câu hỏi: Đứa trẻ đang cầm gì? Trả lời: dơi']\n"
     ]
    }
   ],
   "source": [
    "print(translate_en2vi([\"bat\"]))\n",
    "print(translate_en2vi([\"Question: What is the child holding? Answer: bat\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random 500 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu 500 mẫu vào file sample_500_train.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Đường dẫn đến file dữ liệu train\n",
    "train_file = '../../../datasets/VQA-X/vqaX_train.json'\n",
    "\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "sample_keys = random.sample(list(train_data.keys()), 500)\n",
    "\n",
    "# Tạo danh sách các mẫu\n",
    "samples = []\n",
    "for key in sample_keys:\n",
    "    item = train_data[key]\n",
    "    samples.append({\n",
    "        'id': key,\n",
    "        'question': item['question'],\n",
    "        'question_vi': '',  # Cột để điền bản dịch\n",
    "        'explanation': item['explanation'][0],\n",
    "        'explanations_vi': ''  # Cột để điền bản dịch\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(samples)\n",
    "\n",
    "output_file = 'sample_500_train.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Đã lưu 500 mẫu vào file {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu 32886 câu trả lời duy nhất vào file vqax_answer_gemini.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Đường dẫn đến các file dữ liệu\n",
    "datasets_dir = '../../../datasets/VQA-X'\n",
    "dataset_files = ['vqaX_train_translated.json', 'vqaX_val_translated.json', 'vqaX_test_translated.json']\n",
    "\n",
    "# Hàm để lấy tất cả các câu trả lời từ một file\n",
    "def get_answers_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    answers = []\n",
    "    for key, item in data.items():\n",
    "        answers.append((key, item['answer_vi_gemini']))\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# Lấy tất cả các câu trả lời từ tất cả các file\n",
    "all_answers = []\n",
    "for file in dataset_files:\n",
    "    file_path = f\"{datasets_dir}/{file}\"\n",
    "    all_answers.extend(get_answers_from_file(file_path))\n",
    "\n",
    "sorted_answers = sorted(all_answers, key=lambda x: x[1])\n",
    "df = pd.DataFrame(sorted_answers, columns=['id', 'answer_vi_gemini'])\n",
    "output_file = 'vqax_answer_gemini.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Đã lưu {len(sorted_answers)} câu trả lời duy nhất vào file {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
